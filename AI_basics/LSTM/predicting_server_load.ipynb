{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting requests\n",
    "\n",
    "## Loading data\n",
    "\n",
    "The data shows hourly requests cumulatively over five weeks, starting on Monday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "#Load\n",
    "requests = pd.read_csv('requests_every_hour.csv',header=0)\n",
    "\n",
    "#Overview\n",
    "print(requests.dtypes)\n",
    "requests.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing data\n",
    "\n",
    "The first step is to explore what the data looks like. Are there cyclical patterns, seasonal patterns, trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#one day\n",
    "plt.figure(figsize=(20,5)).suptitle(\"Day\", fontsize=20)\n",
    "plt.plot(requests.head(24))\n",
    "plt.show()\n",
    "\n",
    "#one week\n",
    "plt.figure(figsize=(20,5)).suptitle(\"Week\", fontsize=20)\n",
    "plt.plot(requests.head(168))\n",
    "plt.show()\n",
    "\n",
    "#overall\n",
    "plt.figure(figsize=(20,10)).suptitle(\"Overall\", fontsize=20)\n",
    "plt.plot(requests)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excursus: autocorrelation\n",
    "\n",
    "In time series analysis, autocorrelation can provide information about the extent to which previous values influence the next x values (a = number of lags). Here, we can also see the periodic/seasonal dependency of the values in the correlation.</br>\n",
    "https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plot function\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Autocorrelation plot\n",
    "plot_acf(requests['Requests'], lags = 48)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial autocorrelation plot\n",
    "plot_pacf(requests['Requests'], lags = 48, method='ols')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Periodic data with and without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sinusoidal time series without noise\n",
    "np.random.seed(42)\n",
    "date_rng = pd.date_range(start='2023-01-01', end='2023-01-31', freq='H')  # Hourly data for a month\n",
    "sinusoidal_pattern = np.sin(2 * np.pi * date_rng.hour / 24) * 20\n",
    "df = pd.DataFrame(data={'datetime': date_rng, 'sinusoidal': sinusoidal_pattern})\n",
    "\n",
    "# Set the 'datetime' column as the index\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "plt.title('Raw data')\n",
    "plt.plot(df)\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation plot\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sm.graphics.tsa.plot_acf(df['sinusoidal'], lags=48, ax=ax)  # Assuming hourly data, so lags=24 for 1-day period\n",
    "plt.title('Autocorrelation Plot')\n",
    "plt.show()\n",
    "\n",
    "# Partial autocorrelation plot\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sm.graphics.tsa.plot_pacf(df['sinusoidal'], lags=48, method='ols', ax=ax)\n",
    "plt.title('Partial Autocorrelation Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a strongly seasonal time series dataset\n",
    "np.random.seed(42)\n",
    "date_rng = pd.date_range(start='2023-01-01', end='2023-01-31', freq='H')  # Hourly data for a month\n",
    "daily_seasonality = np.sin(2 * np.pi * date_rng.hour / 24) * 20\n",
    "sales = 100 + daily_seasonality + np.random.normal(0, 5, size=len(date_rng))\n",
    "\n",
    "df = pd.DataFrame(data={'date': date_rng, 'sales': sales})\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "plt.title('Raw data')\n",
    "plt.plot(df)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Autocorrelation plot\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sm.graphics.tsa.plot_acf(df['sales'], lags=40, ax=ax)\n",
    "plt.title('Autocorrelation Plot')\n",
    "plt.show()\n",
    "\n",
    "# Partial autocorrelation plot\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sm.graphics.tsa.plot_pacf(df['sales'], lags=40, ax=ax, method='ols')\n",
    "plt.title('Partial Autocorrelation Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Scale data\n",
    "print(\"Enquiries Range before scaling:\" , \n",
    "          min(requests.Requests),\n",
    "          max(requests.Requests))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_requests=scaler.fit_transform(requests)\n",
    "print(\"Enquiries Area after scaling:\" , \n",
    "          min(scaled_requests),\n",
    "          max(scaled_requests))\n",
    "\n",
    "#It is important to map periodic signals with full periods.\n",
    "train_size = 24 * 7 * 4 #672\n",
    "\n",
    "#How far back do we want to look? One week here\n",
    "lookback = 24 * 7  #168\n",
    "\n",
    "#We can't split this up at random, otherwise the temporal context will be lost.\n",
    "#So we take the first 4 weeks as training data --> 672 data points.\n",
    "train_requests = scaled_requests[0:train_size,:]\n",
    "\n",
    "#Test data is the rest of the data plus the last week of training data to make predictions\n",
    "test_requests = scaled_requests[train_size-lookback:,:]\n",
    "\n",
    "print(\"\\nForm von scaled_request : \", scaled_requests.shape)\n",
    "print(\"\\nForm von train_requests, test_requests : \",\n",
    "      train_requests.shape, test_requests.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to prepare the data set for the LSTM\n",
    "#Each data point (X) is linked to the previous data points of size=lookback.\n",
    "#The predicted value (Y) is the next point.\n",
    "def create_lstm_dataset(data, lookback = 1):\n",
    "    #create two empty lists\n",
    "    data_x, data_y = [], []\n",
    "    for i in range(len(data) - lookback - 1):\n",
    "            #All points from this point backwards for the lookback period\n",
    "            a = data[i:(i + lookback), 0]\n",
    "            data_x.append(a)\n",
    "            #next datapoint\n",
    "            data_y.append(data[i + lookback, 0])\n",
    "    return np.array(data_x), np.array(data_y)\n",
    "\n",
    "#Create X and Y for training\n",
    "train_req_x, train_req_y = create_lstm_dataset(train_requests, lookback)\n",
    "\n",
    "print(\"Form von train_req_x, train_req_y: \",train_req_x.shape, train_req_y.shape)\n",
    "\n",
    "#Transform training data for LSTM\n",
    "train_req_x = np.reshape(train_req_x, (train_req_x.shape[0],1, train_req_x.shape[1]))\n",
    "\n",
    "print(\"Form von train_req_x, train_req_y: \",train_req_x.shape, train_req_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the actual LSTM with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe you need to install tensorflow\n",
    "#!pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "#Create model\n",
    "ts_model=Sequential()\n",
    "\n",
    "#Add LSTM layer\n",
    "ts_model.add(LSTM(256, input_shape=(1,lookback)))\n",
    "#Output layer --> condense to 1 output\n",
    "ts_model.add(Dense(1))\n",
    "\n",
    "#Compiling with Adam Optimizer. Optimising for minimum mean square error\n",
    "ts_model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "#Output model\n",
    "ts_model.summary()\n",
    "\n",
    "#Train model\n",
    "ts_model.fit(train_req_x, train_req_y, epochs=10, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background info:\n",
    "\n",
    "### Batch size: </br>\n",
    "The batch size is a hyperparameter that determines the number of samples that are run before the internal model parameters are updated.\n",
    "\n",
    "Think of a batch as a for loop that iterates over one or more samples and makes predictions. At the end of the batch, the predictions are compared to the expected output variables and an error is calculated. This error is used by the update algorithm to improve the model.\n",
    "\n",
    "A training dataset can be divided into one or more batches.\n",
    "\n",
    "### Epoch: </br>\n",
    "The number of epochs is a hyperparameter that determines how many times the learning algorithm will go through the entire training data set.\n",
    "\n",
    "One epoch means that each sample in the training data set has had a chance to update the internal model parameters. An epoch consists of one or more batches. For example, one epoch with one batch is called batch gradient descent learning algorithm.\n",
    "\n",
    "### What is the difference between batch and epoch?</br>\n",
    "The batch size is the number of samples processed before the model is updated.\n",
    "\n",
    "The number of epochs is the number of complete passes through the training data set.\n",
    "\n",
    "The batch size must be greater than or equal to one and less than or equal to the number of samples in the training data set.\n",
    "\n",
    "The number of epochs can be set to any integer value between one and infinity. You can let the algorithm run for as long as you like and even stop it based on criteria other than a fixed number of epoch, such as a change (or lack thereof) in the model error over time.</br></br>\n",
    "<a href=https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch>https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the test data set in the same way as the training data set.\n",
    "test_req_x, test_req_y = create_lstm_dataset(test_requests,lookback)\n",
    "test_req_x = np.reshape(test_req_x, \n",
    "                         (test_req_x.shape[0],1, test_req_x.shape[1]))\n",
    "\n",
    "#Evaluate model\n",
    "ts_model.evaluate(test_req_x, test_req_y, verbose=1)\n",
    "\n",
    "#Calculate prediction for training data set\n",
    "predict_on_train= ts_model.predict(train_req_x)\n",
    "#Calculate prediction for test data set\n",
    "predict_on_test = ts_model.predict(test_req_x)\n",
    "\n",
    "#Rescale data to compare with original\n",
    "predict_on_train = scaler.inverse_transform(predict_on_train)\n",
    "predict_on_test = scaler.inverse_transform(predict_on_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Total size x-axis\n",
    "total_size = len(predict_on_train) + len(predict_on_test)\n",
    "\n",
    "#Original data \n",
    "orig_data=requests.Requests.to_numpy()\n",
    "#Reformatting in (number of points, 1) for the plot\n",
    "orig_data=orig_data.reshape(len(orig_data),1)\n",
    "#Create an ‘empty’ array for plotting\n",
    "orig_plot = np.empty((total_size,1))\n",
    "#Init array\n",
    "orig_plot[:, :] = np.nan\n",
    "#Transfer data\n",
    "orig_plot[0:total_size, :] = orig_data[lookback:-2,]\n",
    "\n",
    "#Data for training forecasts.\n",
    "predict_train_plot = np.empty((total_size,1))\n",
    "predict_train_plot[:, :] = np.nan\n",
    "predict_train_plot[0:len(predict_on_train), :] = predict_on_train\n",
    "\n",
    "#Data for test forecasts.\n",
    "predict_test_plot = np.empty((total_size,1))\n",
    "predict_test_plot[:, :] = np.nan\n",
    "predict_test_plot[len(predict_on_train):total_size, :] = predict_on_test\n",
    "\n",
    "#Output all data\n",
    "plt.figure(figsize=(20,10)).suptitle(\"Predictions for original, training and test data\", fontsize=20)\n",
    "plt.plot(orig_plot, label=\"Original\")\n",
    "plt.plot(predict_train_plot, label=\"Training\")\n",
    "plt.plot(predict_test_plot,label=\"Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the future\n",
    "\n",
    "The model created can predict the next value in the time series if we provide the previous 168 actual values. This means that it can only predict the first hour of next week. To predict the second hour, we need the actual value for the first hour. So how can we predict a whole week? \n",
    "<p>\n",
    "To account for the missing value in the look-back sequence, we can use the predicted value in the look-back. So for the first hour's prediction, P1, we use the last 168 values of the actual data. For the next hour, P2, we then use our first prediction P1 together with the last 167 values of the actual data. This way, we get contiguous values for the previous sequence, even when one of them is predicted. We can continue along the same lines, adding the current prediction to the look-back for the next prediction.</p><p>\n",
    "Note that as you use more and more prediction instead of actual values in the input, the output will become less and less accurate and after a certain time the patterns can no longer be maintained. It is therefore recommended not to make predictions for a future sequence whose size is greater than the size of the lookback. In this case, both are one week. </p>\n",
    "\n",
    "| Prediction | Lookback |\n",
    "|:--------------|:-------------------------|\n",
    "| P1 | A168 - A1 |\n",
    "| P2 | P1, A168 - A2 |\n",
    "| P3 | P2 - P1, A168 - A3 |\n",
    "| P4 | P3 -P1, A168 - A4 |\n",
    "| P5 | P4 - P1, A168 - A5 |\n",
    "\n",
    "<p>\n",
    "The following code block implements the logic for creating lookbacks with predictions. \n",
    "Then, we perform an inverse transform to scale the values back. Finally, we plot training, test and future prediction in a single diagram. The orange line shows the predictions for four weeks of training data. The green line shows the predictions for one week of test data. The red line refers to one week in the future, using the predicted values and lookbacks. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the last part of the training data as the first review\n",
    "curr_input= test_req_x[-1,:].flatten()\n",
    "\n",
    "#Forecast for next week\n",
    "predict_for = 24 * 7\n",
    "\n",
    "for i in range(predict_for):\n",
    "    \n",
    "    #Let X be the number of the last lookbacks.\n",
    "    this_input=curr_input[-lookback:]\n",
    "    #Generate input\n",
    "    this_input=this_input.reshape((1,1,lookback))\n",
    "    #Predict next datapoint\n",
    "    this_prediction=ts_model.predict(this_input)\n",
    "\n",
    "    #Add the current prediction to the input\n",
    "    curr_input = np.append(curr_input,this_prediction.flatten())\n",
    "    \n",
    "#Extract the last part of predict_for from curr_input that contains all new predictions.\n",
    "predict_on_future=np.reshape(np.array(curr_input[-predict_for:]),(predict_for,1))\n",
    "\n",
    "#Scale back\n",
    "predict_on_future=scaler.inverse_transform(predict_on_future)\n",
    "\n",
    "#show the next ten datapoints\n",
    "print(predict_on_future[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot the training data with the forecast data\n",
    "total_size = len(predict_on_train) + len(predict_on_test) + len(predict_on_future)\n",
    "\n",
    "#Setup training chart\n",
    "predict_train_plot = np.empty((total_size,1))\n",
    "predict_train_plot[:, :] = np.nan\n",
    "predict_train_plot[0:len(predict_on_train), :] = predict_on_train\n",
    "\n",
    "#Setup test chart\n",
    "predict_test_plot = np.empty((total_size,1))\n",
    "predict_test_plot[:, :] = np.nan\n",
    "predict_test_plot[len(predict_on_train):len(predict_on_train)+len(predict_on_test), :] = predict_on_test\n",
    "\n",
    "#Setup future forecast chart\n",
    "predict_future_plot = np.empty((total_size,1))\n",
    "predict_future_plot[:, :] = np.nan\n",
    "predict_future_plot[len(predict_on_train)+len(predict_on_test):total_size, :] = predict_on_future\n",
    "\n",
    "plt.figure(figsize=(20,10)).suptitle(\"Predictions for original, training and test data\", fontsize=20)\n",
    "plt.plot(orig_plot, label=\"Original\")\n",
    "plt.plot(predict_train_plot, label=\"Training\")\n",
    "plt.plot(predict_test_plot,label=\"Test\")\n",
    "plt.plot(predict_future_plot,label=\"Future\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
